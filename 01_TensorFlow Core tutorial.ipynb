{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow核心教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可能会认为TensorFlow Core程序由两个独立部分组成：  \n",
    "- 构建计算图。\n",
    "- 运行计算图。\n",
    "\n",
    "甲计算图形是一系列排列成节点的图形TensorFlow操作。我们来构建一个简单的计算图。每个节点采用零个或多个张量作为输入，并产生张量作为输出。一种类型的节点是一个常数。像所有TensorFlow常数一样，它不需要任何输入，它输出一个内部存储的值。我们可以创建两个浮点式传感器node1 ，node2如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(), dtype=float32) Tensor(\"Const_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0,tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1,node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，打印节点不会输出值3.0，4.0正如您所期望的那样。相反，它们是在评估时分别产生3.0和4.0的节点。要实际评估节点，我们必须在会话中运行计算图。会话封装了TensorFlow运行时的控制和状态。\n",
    "下面的代码创建一个Session对象，然后调用其run方法运行足够的计算图来评价node1和node2。通过在会话中运行计算图如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1,node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过将Tensor节点与操作相结合来构建更复杂的计算（操作也是节点）。例如，我们可以添加我们的两个常量节点并生成一个新的图，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3:  Tensor(\"Add_1:0\", shape=(), dtype=float32)\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3: \", node3)\n",
    "print(\"sess.run(node3): \",sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow提供了一个名为TensorBoard的实用程序，可以显示计算图的图片。这是一个屏幕截图，显示TensorBoard如何可视化图形：  \n",
    "<img src=\"https://raw.githubusercontent.com/mokundong/images/master/t_1.jpg\" alt=\"GitHub\" title=\"GitHub,Social Coding\" width=\"150\" height=\"150\" />\n",
    "就这样，这个图并不是特别有趣，因为它总是产生一个恒定的结果。可以将图形参数化为接受外部输入，称为占位符。一个占位符是一个承诺后提供一个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的三行有点像一个函数或一个lambda，其中我们定义了两个输入参数（a和b），然后对它们进行一个操作。我们可以使用feed_dict参数来指定多个输入的图表来指定为这些占位符提供具体值的Tensors："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b:4.5}))\n",
    "print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorBoard中，图形如下所示：  \n",
    "<img src=\"https://raw.githubusercontent.com/mokundong/images/master/t_2.jpg\" alt=\"GitHub\" title=\"GitHub,Social Coding\" width=\"150\" height=\"150\" /> \n",
    "我们可以通过添加另一个操作来使计算图更加复杂。例如:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b:4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的计算图在TensorBoard中将如下所示：  \n",
    "<img src=\"https://raw.githubusercontent.com/mokundong/images/master/t_3.jpg\" alt=\"GitHub\" title=\"GitHub,Social Coding\" width=\"150\" height=\"150\" />  \n",
    "在机器学习中，我们通常会想要一个可以接受任意输入的模型，比如上面的一个。为了使模型可训练，我们需要能够修改图形以获得具有相同输入的新输出。 变量允许我们向图中添加可训练的参数。它们的构造类型和初始值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常数被调用时初始化tf.constant，其值永远不会改变。相比之下，调用时，变量不会被初始化tf.Variable。要初始化TensorFlow程序中的所有变量，必须显式调用特殊操作，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要的是实现initTensorFlow子图的一个句柄，初始化所有的全局变量。在我们调用之前sess.run，变量未初始化。\n",
    "既然x是占位符，我们可以同时评估linear_model几个值， x如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们创建了一个模型，但是我们不知道它有多好。为了评估培训数据的模型，我们需要一个y占位符来提供所需的值，我们需要编写一个损失函数。\n",
    "损失函数测量当前模型与提供的数据之间的距离。我们将使用线性回归的标准损失模型，其将当前模型和提供的数据之间的三角形的平方相加。linear_model - y创建一个向量，其中每个元素都是对应的示例的错误增量。我们打电话tf.square给这个错误。然后，我们求和所有平方误差，创建一个单一的标量，使用tf.reduce_sum以下方法抽象出所有示例的错误："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以手动重新分配的值提高这W和b为-1和1变量的值，完美初始化为提供的价值 tf.Variable，但可以使用操作等来改变tf.assign。例如， W=-1并且b=1是我们的模型的最佳参数。我们可以改变W，并 b因此："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W,[-1.])\n",
    "fixb = tf.assign(b,[1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的完整讨论超出了本教程的范围。然而，TensorFlow提供了优化器，缓慢地更改每个变量，以便最大程度地减少损失函数。最简单的优化器是梯度下降。它根据相对于该变量的损失导数的大小修改每个变量。通常，手动计算符号导数是乏味且容易出错的。因此，TensorFlow可以使用该函数自动生成仅给出模型描述的导数tf.gradients。为了简单起见，优化器通常为您做这个。例如:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess.run(init)# reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "    sess.run(train,{x:[1,2,3,4],y:[0,-1,-2,-3]})\n",
    "    \n",
    "print(sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经完成了实际的机器学习！虽然这样做简单的线性回归并不需要太多的TensorFlow核心代码，但更复杂的模型和方法将数据输入到模型中需要更多的代码。因此，TensorFlow为常见的模式，结构和功能提供了更高级别的抽象。我们将在下一节中学习如何使用其中的一些抽象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([-0.21999997], dtype=float32), array([-0.456], dtype=float32), 4.0181446]\n",
      "100 [array([-0.84270465], dtype=float32), array([ 0.53753263], dtype=float32), 0.14287975]\n",
      "200 [array([-0.95284992], dtype=float32), array([ 0.86137295], dtype=float32), 0.012838207]\n",
      "300 [array([-0.98586655], dtype=float32), array([ 0.95844597], dtype=float32), 0.0011535464]\n",
      "400 [array([-0.99576342], dtype=float32), array([ 0.98754394], dtype=float32), 0.00010365112]\n",
      "500 [array([-0.99873012], dtype=float32), array([ 0.99626648], dtype=float32), 9.3124017e-06]\n",
      "600 [array([-0.99961936], dtype=float32), array([ 0.99888098], dtype=float32), 8.3645574e-07]\n",
      "700 [array([-0.99988592], dtype=float32), array([ 0.9996646], dtype=float32), 7.514916e-08]\n",
      "800 [array([-0.99996579], dtype=float32), array([ 0.99989945], dtype=float32), 6.7539112e-09]\n",
      "900 [array([-0.99998969], dtype=float32), array([ 0.99996972], dtype=float32), 6.1273298e-10]\n",
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3],tf.float32)\n",
    "b = tf.Variable([-.3],tf.float32)\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "# trainning data\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [0,-1,-2,-3]\n",
    "# trainning loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "    sess.run(train,{x:x_train,y:y_train})\n",
    "    if i % 100 == 0:\n",
    "            print(i,sess.run([W, b, loss], {x:x_train, y:y_train}))\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个更复杂的程序仍然可以在TensorBoard中可视化   \n",
    "![GIthub](https://raw.githubusercontent.com/mokundong/images/master/t_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.contrib.learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.contrib.learn** 是一个高级_TensorFlow_库，简化了机器学习的机制，其中包括：  \n",
    "- 运行训练循环\n",
    "- 运行评估循环\n",
    "- 管理数据集\n",
    "- 管理喂养 \n",
    "\n",
    "tf.contrib.learn定义了许多常见的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本用法**  \n",
    "注意线性回归程序变得简单得多 tf.contrib.learn："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpyoly21c7\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa1c60439b0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpyoly21c7'}\n",
      "WARNING:tensorflow:From /home/ubuntu/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpyoly21c7/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.25, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1190.21\n",
      "INFO:tensorflow:loss = 0.019036, step = 101 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1299.77\n",
      "INFO:tensorflow:loss = 0.0027299, step = 201 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1258.73\n",
      "INFO:tensorflow:loss = 0.000103412, step = 301 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1246.31\n",
      "INFO:tensorflow:loss = 1.05701e-05, step = 401 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1256.12\n",
      "INFO:tensorflow:loss = 6.36375e-07, step = 501 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1286.85\n",
      "INFO:tensorflow:loss = 3.21408e-08, step = 601 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1284.63\n",
      "INFO:tensorflow:loss = 2.08842e-09, step = 701 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1574.16\n",
      "INFO:tensorflow:loss = 4.25848e-10, step = 801 (0.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 2097.62\n",
      "INFO:tensorflow:loss = 1.9984e-11, step = 901 (0.048 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpyoly21c7/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 7.99361e-13.\n",
      "WARNING:tensorflow:From /home/ubuntu/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-31-14:25:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpyoly21c7/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-31-14:26:00\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1.25411e-12\n",
      "{'loss': 1.2541079e-12, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "# Declare list of features. We only have one real-valued feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n",
    " \n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# logistic regression, linear classification, logistic classification, and\n",
    "# many neural network classifiers and regressors. The following code\n",
    "# provides an estimator that does linear regression.\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\n",
    " \n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use `numpy_input_fn`. We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4,\n",
    "                                              num_epochs=1000)\n",
    " \n",
    "# We can invoke 1000 training steps by invoking the `fit` method and passing the\n",
    "# training data set.\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    " \n",
    "# Here we evaluate how well our model did. In a real example, we would want\n",
    "# to use a separate validation and testing data set to avoid overfitting.\n",
    "print(estimator.evaluate(input_fn=input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.contrib.learn不会锁定您的预定义模型。假设我们想创建一个没有内置到TensorFlow中的自定义模型。我们仍然可以保留数据集，饲养，培训等的高层抽象 tf.contrib.learn。为了说明，我们将展示如何实现我们自己的等效模型，以LinearRegressor使用我们对较低级别TensorFlow API的了解。  \n",
    "\n",
    "要定义一个适合的自定义模型tf.contrib.learn，我们需要使用 tf.contrib.learn.Estimator。tf.contrib.learn.LinearRegressor实际上是一个子类tf.contrib.learn.Estimator。Estimator我们只是提供Estimator一个功能model_fn来告诉 tf.contrib.learn它如何评估预测，训练步骤和损失，而不是分类 。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpm18i08nd\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa1c5d37828>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpm18i08nd'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpm18i08nd/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.872889114814, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1084.29\n",
      "INFO:tensorflow:loss = 0.021092450254, step = 101 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1280.25\n",
      "INFO:tensorflow:loss = 0.00278266649174, step = 201 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1257.84\n",
      "INFO:tensorflow:loss = 0.000530643649974, step = 301 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1273.91\n",
      "INFO:tensorflow:loss = 2.26371315863e-05, step = 401 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1294.99\n",
      "INFO:tensorflow:loss = 2.62959274204e-06, step = 501 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1310.27\n",
      "INFO:tensorflow:loss = 1.03383681436e-07, step = 601 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1320.58\n",
      "INFO:tensorflow:loss = 1.54771168278e-08, step = 701 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1523.83\n",
      "INFO:tensorflow:loss = 1.49100350904e-09, step = 801 (0.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 2191.58\n",
      "INFO:tensorflow:loss = 7.85344786771e-11, step = 901 (0.046 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpm18i08nd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.46785044043e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-31-14:28:58\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpm18i08nd/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-31-14:28:58\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 8.58643e-12\n",
      "{'loss': 8.5864284e-12, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Declare list of features, we only have one real-valued feature\n",
    "def model(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W*features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # ModelFnOps connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.contrib.learn.ModelFnOps(\n",
    "      mode=mode, predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    " \n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "# define our data set\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs=1000)\n",
    " \n",
    "# train\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "# evaluate our model\n",
    "print(estimator.evaluate(input_fn=input_fn, steps=10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
